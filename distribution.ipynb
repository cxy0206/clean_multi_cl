{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab090db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model.featurisation import smiles2graph\n",
    "from model.CL_model_vas_info import GNNModelWithNewLoss\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03a7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/vsa.csv\")  \n",
    "smiles_list = df[\"SMILES\"].tolist()\n",
    "smr_vsa_list = [list(map(float, row.split())) for row in df[\"SMR_VSA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vsa_data(vsa_file):\n",
    "    df = pd.read_csv(vsa_file)\n",
    "\n",
    "    def parse_vsa(s):\n",
    "        try:\n",
    "            return list(map(float, s.strip('[]').split()))\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    smr_arrays = df[\"SMR_VSA\"].apply(parse_vsa).tolist()          \n",
    "    slogp_arrays = df[\"SlogP_VSA\"].apply(parse_vsa).tolist()     \n",
    "    peoe_arrays = df[\"PEOE_VSA\"].apply(parse_vsa).tolist()       \n",
    "\n",
    "    properties = list(zip(smr_arrays, slogp_arrays, peoe_arrays))\n",
    "    \n",
    "    return df[\"SMILES\"].tolist(), properties\n",
    "\n",
    "x_smiles, properties = read_vsa_data(\"./data/vsa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990e088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 79], edge_index=[2, 44], edge_attr=[44, 10], global_features=[5], smiles='Cc1cccc(C2=CCN(C(=O)NCCCC#N)CC2)c1', property_0=[1, 10], property_1=[1, 10], property_2=[1, 14])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = smiles2graph(\n",
    "    x_smiles, y=None, cluster=None, properties=properties, test=False\n",
    ")\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa538399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "devices = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n",
    "model1 = GNNModelWithNewLoss(\n",
    "        num_node_features=data_list[0].x.shape[1],\n",
    "        num_edge_features=data_list[0].edge_attr.shape[1],\n",
    "        num_global_features=data_list[0].global_features.shape[0],\n",
    "        hidden_dim=512,\n",
    "        dropout_rate=0.1,\n",
    "        property_index=1 ,\n",
    "        save_path= 'premodels/0' \n",
    "    ).to(devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aaf00f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenxinyi\\AppData\\Local\\Temp\\ipykernel_37412\\2666661902.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(f'premodels/{0}/best_model.pth', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNNModelWithNewLoss(\n",
       "  (conv1): GATConv(79, 512, heads=1)\n",
       "  (conv2): GATConv(512, 512, heads=1)\n",
       "  (conv3): GATConv(512, 512, heads=1)\n",
       "  (bn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (bn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (bn3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (projection_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = devices[0]\n",
    "ckpt = torch.load(f'premodels/{0}/best_model.pth', map_location=device)\n",
    "model1.load_state_dict(ckpt['encoder_state_dict'])\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6d8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def compute_distances(model, dataloader, anchor_smiles=\"O\", vsa_property='property_0', weight_vsa=0.5, weight_embedding=0.5):\n",
    "    \"\"\"\n",
    "    计算每个分子与锚点（水分子）之间的表示距离\n",
    "    vsa_property: 选择的VSA分量，可以是 'property_0', 'property_1', 或 'property_2'\n",
    "    \"\"\"\n",
    "    anchor_embedding = None\n",
    "    anchor_vsa = None\n",
    "    for data in dataloader:\n",
    "        data = data.to(model.device)\n",
    "        prop = model.get_property(data)\n",
    "        embedding = model._project(model.forward(data))\n",
    "\n",
    "        if \"O\" in data.smiles:  # 如果找到水分子\n",
    "            anchor_embedding = embedding\n",
    "            anchor_vsa = prop[vsa_property]\n",
    "            break\n",
    "    else:\n",
    "        # 如果没有找到水分子（'O'），则选择第一个分子，并打印它的SMILES\n",
    "        data = dataloader.dataset[0]\n",
    "        anchor_smiles = data.smiles\n",
    "        print(f\"Water molecule (O) not found. Using the first molecule: {anchor_smiles}\")\n",
    "        anchor_embedding = model._project(model.forward(data.to(model.device)))\n",
    "        anchor_vsa = model.get_property(data)\n",
    "    embedding_distances = []\n",
    "    vsa_distances = []\n",
    "    for data in dataloader:\n",
    "        data = data.to(model.device)\n",
    "        prop = model.get_property(data)\n",
    "        embedding = model._project(model.forward(data))\n",
    "\n",
    "        # 计算embedding距离（余弦相似度）\n",
    "        embedding_dist = 1 - cosine_similarity(anchor_embedding.cpu().detach().numpy(), embedding.cpu().detach().numpy())\n",
    "        embedding_distances.append(embedding_dist.flatten())\n",
    "\n",
    "        # 计算VSA距离（余弦相似度）\n",
    "        anchor_vsa_cpu = anchor_vsa.cpu().detach().numpy().reshape(1, -1)\n",
    "        prop_vsa_cpu = prop.cpu().detach().numpy().reshape(1, -1)\n",
    "        vsa_dist = 1 - cosine_similarity(anchor_vsa_cpu, prop_vsa_cpu)\n",
    "        vsa_distances.append(vsa_dist.flatten())\n",
    "\n",
    "    # 将embedding距离和VSA距离进行标准化\n",
    "    scaler = StandardScaler()\n",
    "    embedding_distances_scaled = scaler.fit_transform(np.array(embedding_distances).reshape(-1, 1)).flatten()\n",
    "    vsa_distances_scaled = scaler.fit_transform(np.array(vsa_distances).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    return  np.array(vsa_distances_scaled), np.array(embedding_distances_scaled )\n",
    "   \n",
    "\n",
    "# 改进散点图的可视化\n",
    "def plot_distances_vs_vsa(embedding_distances, vsa_distances, vsa_label):\n",
    "    \"\"\"\n",
    "    绘制距离与VSA分量的关系图\n",
    "    vsa_label: VSA分量的标签，用于绘制标题\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(embedding_distances, vsa_distances, alpha=0.7, edgecolors='w', s=30, cmap='viridis')\n",
    "    plt.xlabel('Embedding Distance to Anchor Molecule (Water)')\n",
    "    plt.ylabel(f'VSA Component ({vsa_label}) Distance')\n",
    "    plt.title(f'Embedding Distance vs VSA Component ({vsa_label}) Distance')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 聚类并改进图形显示\n",
    "# 聚类并打印最靠近中心的10个分子的SMILES\n",
    "def cluster_and_plot(embedding_distances, vsa_distances, n_clusters=20, dataloader=None):\n",
    "    # 标准化距离以进行聚类\n",
    "    scaler = StandardScaler()\n",
    "    distances_scaled = scaler.fit_transform(embedding_distances.reshape(-1, 1))\n",
    "\n",
    "    # 使用KMeans进行聚类\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(distances_scaled)\n",
    "\n",
    "    # 绘制聚类结果\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(embedding_distances, vsa_distances, c=kmeans.labels_, cmap='viridis', alpha=0.6, edgecolors='w', s=30)\n",
    "    plt.xlabel('Embedding Distance to Anchor Molecule (Water)')\n",
    "    plt.ylabel('VSA Distance')\n",
    "    plt.title('Molecule Clusters Based on Embedding Distance and VSA Distance')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 对于每个聚类，获取最靠近中心的10个分子并输出其SMILES\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "        cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "        cluster_distances = embedding_distances[cluster_indices]\n",
    "        \n",
    "        # 找到最靠近中心的10个分子\n",
    "        closest_indices = cluster_indices[np.argsort(np.abs(cluster_distances - cluster_center))[:10]]\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        \n",
    "        # 输出每个最靠近中心的分子的SMILES和其他信息\n",
    "        for idx in closest_indices:\n",
    "            smiles = dataloader.dataset[idx].smiles  # 获取该分子的SMILES\n",
    "            print(f\"  Molecule index: {idx} (SMILES: {smiles}, Embedding Distance: {embedding_distances[idx]}, VSA Distance: {vsa_distances[idx]})\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e213fe3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_distances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 示例\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m embedding_distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43membedding_distances\u001b[49m)\n\u001b[0;32m     34\u001b[0m vsa_distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vsa_distances)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 计算相关性\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_distances' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 计算Embedding和VSA之间的皮尔逊相关系数\n",
    "def compute_correlation(embedding_distances, vsa_distances):\n",
    "    correlation_matrix = np.corrcoef(embedding_distances, vsa_distances)\n",
    "    print(f\"Correlation coefficient between Embedding and VSA: {correlation_matrix[0, 1]}\")\n",
    "\n",
    "# 使用PCA降维并可视化\n",
    "def apply_pca_and_visualize(embedding_distances, vsa_distances):\n",
    "    # 将数据标准化\n",
    "    scaler = StandardScaler()\n",
    "    data = np.vstack((embedding_distances, vsa_distances)).T\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # 应用PCA降维\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data_scaled)\n",
    "\n",
    "    # 可视化PCA结果\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title('PCA Visualization of Embedding and VSA Distances')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 示例\n",
    "embedding_distances = np.array(embedding_distances)\n",
    "vsa_distances = np.array(vsa_distances)\n",
    "\n",
    "# 计算相关性\n",
    "compute_correlation(embedding_distances, vsa_distances)\n",
    "\n",
    "# 使用PCA降维并可视化\n",
    "apply_pca_and_visualize(embedding_distances, vsa_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f339dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
    "\n",
    "vsa_property = 'property_0'  # 可以选择 'property_0', 'property_1', 或 'property_2'\n",
    "embedding_distances, vsa_distances = compute_distances(model1, dataloader, anchor_smiles=\"O\", vsa_property=vsa_property)\n",
    "plot_distances_vs_vsa(embedding_distances, vsa_distances, vsa_label=vsa_property)\n",
    "cluster_and_plot(embedding_distances, vsa_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52dc6c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
    "model1.get_distribution(dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
