{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa8c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search freesolv-concat\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (150) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 263\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fm \u001b[38;5;129;01min\u001b[39;00m fusion_methods:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrid search \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 263\u001b[0m     best \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest:\u001b[39m\u001b[38;5;124m'\u001b[39m, best)\n\u001b[0;32m    265\u001b[0m     res \u001b[38;5;241m=\u001b[39m run_multiple(fm, ds, best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 201\u001b[0m, in \u001b[0;36mhyperparam_search\u001b[1;34m(fusion_method, name, grid)\u001b[0m\n\u001b[0;32m    199\u001b[0m sample \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    200\u001b[0m model \u001b[38;5;241m=\u001b[39m get_finetune_model(fusion_method, sample, dropout\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 201\u001b[0m train_losses, val_rmses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_rmses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    203\u001b[0m     best \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: val_rmses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]}\n",
      "Cell \u001b[1;32mIn[2], line 120\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m    118\u001b[0m pred \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m out  \u001b[38;5;66;03m# Ensure proper unpacking\u001b[39;00m\n\u001b[0;32m    119\u001b[0m label \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 120\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    122\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:3383\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\chenxinyi\\.conda\\envs\\torch\\lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (150) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to fine-tune GNN models on freesolv, lipo, and esol datasets\n",
    "using three representation fusion methods: concat, weighted average,\n",
    "and transformer-based attention. Uses pre-trained encoders from 'premodels/0-2/'.\n",
    "Performs hyperparameter grid search (excluding lr & emb_dim), selects best hyperparameters,\n",
    "then runs 10 independent trials per method, computes mean/variance of top-3 runs,\n",
    "and plots results.\n",
    "Adds learning rate scheduler and early stopping.\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EMB_DIM = 512\n",
    "PATIENCE_ES = 20  # early stopping patience\n",
    "PATIENCE_LR = 10   # scheduler patience\n",
    "LR_FACTOR = 0.5   # scheduler reduce factor\n",
    "\n",
    "# User modules\n",
    "from model.featurisation import smiles2graph\n",
    "from model.CL_model_vas_info import GNNModelWithNewLoss\n",
    "from model.fusion import TransformerFusionModel, WeightedFusion, MLP, FusionFineTuneModel\n",
    "\n",
    "# Utility: hyperparameter grid\n",
    "def product_dict(grid):\n",
    "    for combo in product(*grid.values()):\n",
    "        yield dict(zip(grid.keys(), combo))\n",
    "\n",
    "# Data loading as in your Jupyter snippet\n",
    "def load_data(name, batch_size=32, val_split=0.1, test_split=0.2, seed=42):\n",
    "    df = pd.read_csv(f'data/{name}.csv')\n",
    "    smiles_list = df['smiles'].tolist()\n",
    "    labels = df[name].tolist()\n",
    "    data_list = smiles2graph(smiles_list, labels)\n",
    "    train_val, test_data = train_test_split(data_list, test_size=test_split, random_state=seed)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val, test_size=val_split/(1 - test_split), random_state=seed\n",
    "    )\n",
    "    train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = GeoDataLoader(val_data,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = GeoDataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Load pre-trained encoders\n",
    "def load_pretrained_encoders(sample):\n",
    "    encoders = []\n",
    "\n",
    "    for i in range(3):\n",
    "        enc = GNNModelWithNewLoss(\n",
    "            num_node_features=sample.x.shape[1],\n",
    "            num_edge_features=sample.edge_attr.shape[1],\n",
    "            num_global_features=sample.global_features.shape[0],\n",
    "            hidden_dim=EMB_DIM\n",
    "        ).to(device)\n",
    "        ckpt = torch.load(f'premodels/{i}/best_model.pth', map_location=device)\n",
    "        enc.load_state_dict(ckpt['encoder_state_dict'])\n",
    "        encoders.append(enc)\n",
    "    return encoders\n",
    "\n",
    "# Build fusion model\n",
    "def get_finetune_model(fusion_method, sample, dropout):\n",
    "    encoders = load_pretrained_encoders(sample)\n",
    "    if fusion_method == 'attention':\n",
    "        fusion = TransformerFusionModel(emb_dim=EMB_DIM).to(device)\n",
    "    elif fusion_method == 'weighted':\n",
    "        fusion = WeightedFusion(num_inputs=3, emb_dim=EMB_DIM, dropout=dropout).to(device)\n",
    "    elif fusion_method == 'concat':\n",
    "        fusion = MLP(emb_dim=EMB_DIM * 3).to(device)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown fusion method {fusion_method}')\n",
    "    return FusionFineTuneModel(encoders, fusion, fusion_method).to(device)\n",
    "\n",
    "# Single training routine\n",
    "# Training loop\n",
    "def train_and_validate(model, train_loader, val_loader, epochs):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=LR_FACTOR, patience=PATIENCE_LR)\n",
    "\n",
    "    train_losses, val_rmses = [], []\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state = None\n",
    "    patience_cnt = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            embs = [encoder(batch) for encoder in model.encoders]  # List of [B, D]\n",
    "            embs = torch.stack(embs, dim=1)  # [B, 3, D]\n",
    "\n",
    "            # Handle fusion method\n",
    "            if model.fusion_method == 'concat':\n",
    "                # Flatten for MLP\n",
    "                embs = embs.view(embs.size(0), -1)  # [B, 3 * D]\n",
    "            \n",
    "            out = model.fusion(embs)  # Fusion output\n",
    "            pred = out[0] if isinstance(out, tuple) else out  # Ensure proper unpacking\n",
    "            label = batch.y.view(-1).float().to(device)\n",
    "            loss = criterion(pred, label).sqrt()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds, labs = [], []\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                embs = [encoder(batch) for encoder in model.encoders]  # List of [B, D]\n",
    "                embs = torch.stack(embs, dim=1)  # [B, 3, D]\n",
    "\n",
    "                if model.fusion_method == 'concat':\n",
    "                    # Flatten for MLP\n",
    "                    embs = embs.view(embs.size(0), -1)  # [B, 3 * D]\n",
    "                \n",
    "                out = model.fusion(embs)  # Fusion output\n",
    "                pred = out[0] if isinstance(out, tuple) else out  # Ensure proper unpacking\n",
    "                preds.append(pred.cpu())\n",
    "                labs.append(batch.y.view(-1).cpu())\n",
    "            preds = torch.cat(preds)\n",
    "            labs = torch.cat(labs)\n",
    "            rmse = criterion(preds, labs).sqrt().item()\n",
    "        val_rmses.append(rmse)\n",
    "        scheduler.step(rmse)\n",
    "\n",
    "        # Early stopping\n",
    "        if rmse < best_val_rmse - 1e-6:\n",
    "            best_val_rmse = rmse\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= PATIENCE_ES:\n",
    "                break\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}] Train Loss={avg_loss:.4f}, Val RMSE={rmse:.4f}\")\n",
    "\n",
    "    # Load best\n",
    "    model.load_state_dict(best_state)\n",
    "    return train_losses, val_rmses\n",
    "\n",
    "\n",
    "# Final test evaluation\n",
    "def test_model(model, test_loader):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, labs = [], []\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            embs = [encoder(batch) for encoder in model.encoders]  # List of [B, D]\n",
    "            embs = torch.stack(embs, dim=1)  # [B, 3, D]\n",
    "\n",
    "            if model.fusion_method == 'concat':\n",
    "                # Flatten for MLP\n",
    "                embs = embs.view(embs.size(0), -1)  # [B, 3 * D]\n",
    "\n",
    "            out = model.fusion(embs)  # Fusion output\n",
    "            pred = out if not isinstance(out, tuple) else out[0]  # Handle single or tuple output\n",
    "            preds.append(pred.cpu())\n",
    "            labs.append(batch.y.view(-1).cpu())\n",
    "        preds = torch.cat(preds)\n",
    "        labs = torch.cat(labs)\n",
    "        rmse = criterion(preds, labs).sqrt().item()\n",
    "    return rmse, preds.numpy(), labs.numpy()\n",
    "\n",
    "# Hyperparameter search\n",
    "def hyperparam_search(fusion_method, name, grid):\n",
    "    best = {'params': None, 'rmse': float('inf')}\n",
    "    for params in product_dict(grid):\n",
    "        tr, vl, _ = load_data(name,\n",
    "            batch_size=params['batch_size'], val_split=params['val_split'],\n",
    "            test_split=params['test_split'], seed=params['seed'])\n",
    "        sample = tr.dataset[0]\n",
    "        model = get_finetune_model(fusion_method, sample, dropout=params['dropout'])\n",
    "        train_losses, val_rmses = train_and_validate(model, tr, vl, epochs=params['epochs'])\n",
    "        if val_rmses[-1] < best['rmse']:\n",
    "            best = {'params': params, 'rmse': val_rmses[-1]}\n",
    "    return best\n",
    "\n",
    "# Multi-run for each best config\n",
    "def run_multiple(fusion_method, name, params, runs=10):\n",
    "    rmses, histories, results = [], [], []\n",
    "    for i in range(runs):\n",
    "        tr, vl, te = load_data(name,\n",
    "            batch_size=params['batch_size'], val_split=params['val_split'],\n",
    "            test_split=params['test_split'], seed=42+i)\n",
    "        sample = tr.dataset[0]\n",
    "        model = get_finetune_model(fusion_method, sample, dropout=params['dropout'])\n",
    "        tr_losses, val_rmses = train_and_validate(model, tr, vl, epochs=params['epochs'])\n",
    "        rmses.append(val_rmses[-1])\n",
    "        histories.append((tr_losses, val_rmses))\n",
    "        rmse_test, preds, labs = test_model(model, te)\n",
    "        results.append((preds, labs))\n",
    "    idx = np.argsort(rmses)[:3]\n",
    "    mean_top3 = np.mean([rmses[i] for i in idx])\n",
    "    var_top3  = np.var ([rmses[i] for i in idx])\n",
    "    best_idx = idx[0]\n",
    "    best_history = histories[best_idx]\n",
    "    best_preds, best_labs = results[best_idx]\n",
    "    return {\n",
    "        'mean_top3': mean_top3,\n",
    "        'var_top3': var_top3,\n",
    "        'best_history': best_history,\n",
    "        'best_preds': best_preds,\n",
    "        'best_labels': best_labs\n",
    "    }\n",
    "\n",
    "# Plot helpers\n",
    "def plot_training(train_losses, val_rmses, ds, method, out_dir):\n",
    "    plt.figure(); plt.plot(train_losses, label='Train Loss'); plt.plot(val_rmses, label='Val RMSE')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Metric'); plt.legend(); plt.title(f'{ds}-{method}')\n",
    "    plt.savefig(f'{out_dir}/{ds}_{method}_train.png'); plt.close()\n",
    "\n",
    "def plot_test_distribution(preds, labs, ds, method, out_dir):\n",
    "    plt.figure(); plt.scatter(labs, preds, alpha=0.5)\n",
    "    mn, mx = labs.min(), labs.max()\n",
    "    plt.plot([mn, mx], [mn, mx], 'k--'); plt.xlabel('True'); plt.ylabel('Pred')\n",
    "    plt.title(f'{ds}-{method} Test'); plt.savefig(f'{out_dir}/{ds}_{method}_test.png'); plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = ['freesolv', 'lipo', 'esol']\n",
    "    fusion_methods = ['concat', 'weighted', 'attention']\n",
    "    param_grid = {\n",
    "        'weight_decay': [0, 1e-5],\n",
    "        'dropout': [0.1, 0.3],\n",
    "        'batch_size': [16, 64],\n",
    "        'epochs': [100, 200],\n",
    "        'val_split': [0.1],\n",
    "        'test_split': [0.2],\n",
    "        'seed': [42]\n",
    "    }\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    summary = []\n",
    "    for ds in datasets:\n",
    "        for fm in fusion_methods:\n",
    "            print(f'Grid search {ds}-{fm}')\n",
    "            best = hyperparam_search(fm, ds, param_grid)\n",
    "            print('Best:', best)\n",
    "            res = run_multiple(fm, ds, best['params'], runs=3)\n",
    "            plot_training(*res['best_history'], ds, fm, 'results')\n",
    "            plot_test_distribution(res['best_preds'], res['best_labels'], ds, fm, 'results')\n",
    "            summary.append({\n",
    "                'dataset': ds, 'fusion': fm, 'params': best['params'],\n",
    "                'mean_top3': res['mean_top3'], 'var_top3': res['var_top3']\n",
    "            })\n",
    "    pd.DataFrame(summary).to_csv('results/summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 000] Train Loss=5.0079, Val RMSE=3.8697\n",
      "[Epoch 001] Train Loss=3.7007, Val RMSE=2.7320\n",
      "[Epoch 002] Train Loss=2.9068, Val RMSE=2.0579\n",
      "[Epoch 003] Train Loss=2.3116, Val RMSE=1.5915\n",
      "[Epoch 004] Train Loss=2.1705, Val RMSE=1.8356\n",
      "[Epoch 005] Train Loss=1.8570, Val RMSE=1.6444\n",
      "[Epoch 006] Train Loss=1.8771, Val RMSE=1.6535\n",
      "[Epoch 007] Train Loss=1.7734, Val RMSE=1.3816\n",
      "[Epoch 008] Train Loss=1.7798, Val RMSE=1.3761\n",
      "[Epoch 009] Train Loss=1.6256, Val RMSE=1.4812\n",
      "[Epoch 010] Train Loss=1.5743, Val RMSE=1.2728\n",
      "[Epoch 011] Train Loss=1.5890, Val RMSE=1.3177\n",
      "[Epoch 012] Train Loss=1.4718, Val RMSE=1.2723\n",
      "[Epoch 013] Train Loss=1.4234, Val RMSE=1.2690\n",
      "[Epoch 014] Train Loss=1.5489, Val RMSE=1.1661\n",
      "[Epoch 015] Train Loss=1.4598, Val RMSE=1.2239\n",
      "[Epoch 016] Train Loss=1.5162, Val RMSE=1.2533\n",
      "[Epoch 017] Train Loss=1.3844, Val RMSE=1.2692\n",
      "[Epoch 018] Train Loss=1.3527, Val RMSE=1.2726\n",
      "[Epoch 019] Train Loss=1.3437, Val RMSE=1.1674\n",
      "[Epoch 020] Train Loss=1.2424, Val RMSE=1.2832\n",
      "[Epoch 021] Train Loss=1.2699, Val RMSE=1.2606\n",
      "[Epoch 022] Train Loss=1.2767, Val RMSE=1.2229\n",
      "[Epoch 023] Train Loss=1.3122, Val RMSE=1.4024\n",
      "[Epoch 024] Train Loss=1.2955, Val RMSE=1.1667\n",
      "[Epoch 025] Train Loss=1.3101, Val RMSE=1.1253\n",
      "[Epoch 026] Train Loss=1.1955, Val RMSE=1.2298\n",
      "[Epoch 027] Train Loss=1.2870, Val RMSE=1.2297\n",
      "[Epoch 028] Train Loss=1.2993, Val RMSE=1.2613\n",
      "[Epoch 029] Train Loss=1.2244, Val RMSE=1.2505\n",
      "[Epoch 030] Train Loss=1.2418, Val RMSE=1.2658\n",
      "[Epoch 031] Train Loss=1.2572, Val RMSE=1.3375\n",
      "[Epoch 032] Train Loss=1.2077, Val RMSE=1.2821\n",
      "[Epoch 033] Train Loss=1.1917, Val RMSE=1.0939\n",
      "[Epoch 034] Train Loss=1.1613, Val RMSE=1.3314\n",
      "[Epoch 035] Train Loss=1.2353, Val RMSE=1.2204\n",
      "[Epoch 036] Train Loss=1.2120, Val RMSE=1.1010\n",
      "[Epoch 037] Train Loss=1.1975, Val RMSE=1.1172\n",
      "[Epoch 038] Train Loss=1.2115, Val RMSE=1.0538\n",
      "[Epoch 039] Train Loss=1.2811, Val RMSE=1.1072\n",
      "[Epoch 040] Train Loss=1.1999, Val RMSE=1.1733\n",
      "[Epoch 041] Train Loss=1.1785, Val RMSE=1.2116\n",
      "[Epoch 042] Train Loss=1.2396, Val RMSE=1.2233\n",
      "[Epoch 043] Train Loss=1.1696, Val RMSE=1.0658\n",
      "[Epoch 044] Train Loss=1.2712, Val RMSE=1.1713\n",
      "[Epoch 045] Train Loss=1.1212, Val RMSE=1.1412\n",
      "[Epoch 046] Train Loss=1.1698, Val RMSE=1.1204\n",
      "[Epoch 047] Train Loss=1.1683, Val RMSE=1.1233\n",
      "[Epoch 048] Train Loss=1.2004, Val RMSE=1.0719\n",
      "[Epoch 049] Train Loss=1.2205, Val RMSE=1.3611\n",
      "[Epoch 050] Train Loss=1.1011, Val RMSE=1.0784\n",
      "[Epoch 051] Train Loss=1.0082, Val RMSE=1.1533\n",
      "[Epoch 052] Train Loss=1.0693, Val RMSE=1.1675\n",
      "[Epoch 053] Train Loss=1.0580, Val RMSE=1.1352\n",
      "[Epoch 054] Train Loss=1.0989, Val RMSE=1.0934\n",
      "[Epoch 055] Train Loss=1.1568, Val RMSE=1.1653\n",
      "[Epoch 056] Train Loss=1.2092, Val RMSE=1.2028\n",
      "[Epoch 057] Train Loss=1.0611, Val RMSE=1.1169\n",
      "Best and worst plots saved to results/attention_results_best_worst_2.pdf\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
