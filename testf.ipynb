{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fixed hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EMB_DIM = 512\n",
    "PATIENCE_ES = 20  # early stopping patience\n",
    "PATIENCE_LR = 10   # scheduler patience\n",
    "LR_FACTOR = 0.5   # scheduler reduce factor\n",
    "\n",
    "# User modules\n",
    "from model.featurisation import smiles2graph\n",
    "from model.CL_model_vas_info import GNNModelWithNewLoss\n",
    "from model.fusion import TransformerFusionModel, WeightedFusion, MLP, FusionFineTuneModel\n",
    "\n",
    "# Data loading\n",
    "def load_data_for_visualization(name, batch_size=32, val_split=0.1, test_split=0.2, seed=42):\n",
    "    df = pd.read_csv(f'data/{name}.csv')\n",
    "    smiles_list = df['smiles'].tolist()\n",
    "    labels = df[name].tolist()\n",
    "    data_list = smiles2graph(smiles_list, labels)\n",
    "    \n",
    "    # Train-test split\n",
    "    train_val, test_data = train_test_split(data_list, test_size=test_split, random_state=seed)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val, test_size=val_split/(1 - test_split), random_state=seed\n",
    "    )\n",
    "    \n",
    "    train_loader = GeoDataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = GeoDataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = GeoDataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Get test dataset smiles\n",
    "    test_smiles_list = [data.smiles for data in test_data]\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, test_smiles_list\n",
    "\n",
    "# Load pre-trained encoders\n",
    "def load_pretrained_encoders(sample, encoders_to_use=[0, 1, 2]):\n",
    "    encoders = []\n",
    "    for i in encoders_to_use:\n",
    "        enc = GNNModelWithNewLoss(\n",
    "            num_node_features=sample.x.shape[1],\n",
    "            num_edge_features=sample.edge_attr.shape[1],\n",
    "            num_global_features=sample.global_features.shape[0],\n",
    "            hidden_dim=EMB_DIM\n",
    "        ).to(device)\n",
    "        ckpt = torch.load(f'premodels/{i}/best_model.pth', map_location=device)\n",
    "        enc.load_state_dict(ckpt['encoder_state_dict'])\n",
    "        encoders.append(enc)\n",
    "    return encoders\n",
    "\n",
    "# Build fusion model based on encoder selection\n",
    "def get_finetune_model(fusion_method, sample, dropout, encoders_to_use=[0, 1, 2]):\n",
    "    encoders = load_pretrained_encoders(sample, encoders_to_use)  # Select encoders based on input\n",
    "    if fusion_method == 'attention':\n",
    "        fusion = TransformerFusionModel(emb_dim=EMB_DIM).to(device)\n",
    "    elif fusion_method == 'weighted':\n",
    "        fusion = WeightedFusion(num_inputs=len(encoders), emb_dim=EMB_DIM, dropout=dropout).to(device)\n",
    "    elif fusion_method == 'concat':\n",
    "        fusion = MLP(emb_dim=EMB_DIM * len(encoders)).to(device)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown fusion method {fusion_method}')\n",
    "    return FusionFineTuneModel(encoders, fusion, fusion_method).to(device)\n",
    "\n",
    "# Training routine\n",
    "def train_and_validate(model, train_loader, val_loader, epochs):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=LR_FACTOR, patience=PATIENCE_LR)\n",
    "\n",
    "    train_losses, val_rmses = [], []\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state = None\n",
    "    patience_cnt = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            embs = [encoder(batch) for encoder in model.encoders]\n",
    "            embs = torch.stack(embs, dim=1)  # [B, N_encoders, D]\n",
    "\n",
    "            # Handle fusion method\n",
    "            if model.fusion_method == 'concat':\n",
    "                embs = embs.view(embs.size(0), -1)  # Flatten for MLP: [B, N_encoders * D]\n",
    "            \n",
    "            out = model.fusion(embs)  # Fusion output\n",
    "            pred = out[0] if isinstance(out, tuple) else out  # Ensure proper unpacking\n",
    "            label = batch.y.view(-1).float().to(device)\n",
    "\n",
    "            # Ensure the shape of pred and label match\n",
    "            pred = pred.view(-1)  # Flatten predictions to match label shape\n",
    "            label = label.view(-1)  # Flatten label to match prediction shape\n",
    "            \n",
    "            loss = criterion(pred, label).sqrt()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds, labs = [], []\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                embs = [encoder(batch) for encoder in model.encoders]\n",
    "                embs = torch.stack(embs, dim=1)\n",
    "\n",
    "                if model.fusion_method == 'concat':\n",
    "                    embs = embs.view(embs.size(0), -1)\n",
    "\n",
    "                out = model.fusion(embs)\n",
    "                pred = out[0] if isinstance(out, tuple) else out\n",
    "                pred = pred.view(-1)  # Flatten predictions to match label shape\n",
    "                label = batch.y.view(-1).cpu()\n",
    "\n",
    "                preds.append(pred.cpu())\n",
    "                labs.append(label)\n",
    "            \n",
    "            preds = torch.cat(preds)\n",
    "            labs = torch.cat(labs)\n",
    "            rmse = criterion(preds, labs).sqrt().item()\n",
    "        val_rmses.append(rmse)\n",
    "        scheduler.step(rmse)\n",
    "\n",
    "        # Early stopping\n",
    "        if rmse < best_val_rmse - 1e-6:\n",
    "            best_val_rmse = rmse\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= PATIENCE_ES:\n",
    "                break\n",
    "\n",
    "        # print(f\"[Epoch {epoch:03d}] Train Loss={avg_loss:.4f}, Val RMSE={rmse:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return train_losses, val_rmses\n",
    "\n",
    "# Final test evaluation\n",
    "def test_model(model, test_loader):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, labs = [], []\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            embs = [encoder(batch) for encoder in model.encoders]\n",
    "            embs = torch.stack(embs, dim=1)\n",
    "\n",
    "            if model.fusion_method == 'concat':\n",
    "                embs = embs.view(embs.size(0), -1)\n",
    "\n",
    "            out = model.fusion(embs)\n",
    "            pred = out if not isinstance(out, tuple) else out[0]\n",
    "            preds.append(pred.cpu())\n",
    "            labs.append(batch.y.view(-1).cpu())\n",
    "        preds = torch.cat(preds)\n",
    "        labs = torch.cat(labs)\n",
    "        rmse = criterion(preds, labs).sqrt().item()\n",
    "    \n",
    "    return rmse, preds.numpy(), labs.numpy()\n",
    "\n",
    "# Direct usage of fixed parameters (no hyperparameter search)\n",
    "def run_multiple(fusion_method, name, params, encoders_to_use, runs=10):\n",
    "    rmses, histories, results = [], [], []\n",
    "    for i in range(runs):\n",
    "        # Get data loaders\n",
    "        tr, vl, te, _ = load_data_for_visualization(name, batch_size=params['batch_size'], val_split=params['val_split'],\n",
    "            test_split=params['test_split'], seed=42+i)\n",
    "        \n",
    "        sample = tr.dataset[0]  # Get a sample batch from the training loader\n",
    "        model = get_finetune_model(fusion_method, sample, dropout=params['dropout'], encoders_to_use=encoders_to_use)\n",
    "        \n",
    "        # Train the model\n",
    "        tr_losses, val_rmses = train_and_validate(model, tr, vl, epochs=params['epochs'])\n",
    "        rmses.append(val_rmses[-1])  # Save the last RMSE of the validation set\n",
    "        histories.append((tr_losses, val_rmses))  # Save history of training and validation\n",
    "        \n",
    "        # Test the model\n",
    "        rmse_test, preds, labs = test_model(model, te)\n",
    "        results.append((preds, labs))  # Save predictions and labels for testing\n",
    "    \n",
    "    # Average the RMSE over all runs\n",
    "    mean_rmse = np.mean(rmses)\n",
    "    var_rmse = np.var(rmses)\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        'mean_rmse': mean_rmse,\n",
    "        'var_rmse': var_rmse,\n",
    "        'all_histories': histories,\n",
    "        'all_preds': [res[0] for res in results],\n",
    "        'all_labels': [res[1] for res in results]\n",
    "    }\n",
    "\n",
    "def run_ablations(best_params, runs=3):\n",
    "    # Define datasets and fusion methods inside this function\n",
    "    datasets = ['freesolv']\n",
    "    fusion_methods = ['concat', 'weighted', 'attention']\n",
    "    \n",
    "    # Define all combinations of encoders to test\n",
    "    encoder_combinations = [\n",
    "        [0, 1, 2],  # All encoders\n",
    "        [0],  # Only encoder 0\n",
    "        [1],  # Only encoder 1\n",
    "        [2],  # Only encoder 2\n",
    "        [0, 1],  # Encoder 0 and 1\n",
    "        [0, 2],  # Encoder 0 and 2\n",
    "        [1, 2]   # Encoder 1 and 2\n",
    "    ]\n",
    "    \n",
    "    # Initialize the result table to store the mean ± var for each encoder combination\n",
    "    result_table = {f\"encoders_{'-'.join(map(str, encoders))}\": [] for encoders in encoder_combinations}\n",
    "    \n",
    "    # Loop through each dataset and fusion method combination\n",
    "    for ds in datasets:\n",
    "        for fusion_method in fusion_methods:\n",
    "            print(f\"Running ablation for dataset: {ds}, fusion method: {fusion_method}\")\n",
    "            \n",
    "            # Loop over each encoder combination\n",
    "            for encoders_to_use in encoder_combinations:\n",
    "                print(f\"Running with encoders: {encoders_to_use}\")\n",
    "                \n",
    "                # Run multiple trials for each configuration\n",
    "                res = run_multiple(fusion_method, ds, best_params, encoders_to_use=encoders_to_use, runs=runs)\n",
    "                \n",
    "                # Compute the mean and variance of RMSE\n",
    "                mean_rmse = res['mean_rmse']\n",
    "                var_rmse = res['var_rmse']\n",
    "                \n",
    "                # Format mean ± var\n",
    "                mean_var = f\"{mean_rmse:.4f} ± {var_rmse:.4f}\"\n",
    "                \n",
    "                # Append the result for this encoder configuration\n",
    "                result_table[f\"encoders_{'-'.join(map(str, encoders_to_use))}\"].append(mean_var)\n",
    "    \n",
    "    # Construct the final result table (summary)\n",
    "    summary_data = []\n",
    "    \n",
    "    # For each fusion method (only 1 method at a time), add a row for each dataset\n",
    "    for fusion_method in fusion_methods:\n",
    "        for ds in datasets:\n",
    "            row = [ds, fusion_method]  # First two columns: dataset and fusion method\n",
    "            \n",
    "            # Append results for each encoder combination (mean ± var)\n",
    "            for encoders in encoder_combinations:\n",
    "                encoder_key = f\"encoders_{'-'.join(map(str, encoders))}\"\n",
    "                row.append(result_table[encoder_key][datasets.index(ds)])  # Match dataset index\n",
    "\n",
    "            summary_data.append(row)\n",
    "    \n",
    "    # Save the final result to a single CSV file\n",
    "    summary_df = pd.DataFrame(\n",
    "        summary_data, \n",
    "        columns=[\"Dataset\", \"Fusion Method\"] + [f\"Encoders_{'-'.join(map(str, encoders))}\" for encoders in encoder_combinations]\n",
    "    )\n",
    "    summary_df.to_csv(f\"abl_result/{fusion_method}_ablation_summary.csv\", index=False)\n",
    "    print(f\"Results saved to abl_result/{fusion_method}_ablation_summary.csv\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Directly using fixed parameters (no hyperparameter search)\n",
    "    best_params = {\n",
    "        'batch_size': 16,  # Fixed batch size\n",
    "        'dropout': 0.1,    # Fixed dropout\n",
    "        'epochs': 100,     # Fixed number of epochs\n",
    "        'val_split': 0.1,  # Validation split\n",
    "        'test_split': 0.2, # Test split\n",
    "        'seed': 42,        # Random seed\n",
    "        'weight_decay': 0  # Weight decay (regularization)\n",
    "    }\n",
    "\n",
    "    # Run the ablation study with the given parameters\n",
    "    run_ablations(best_params, runs=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
